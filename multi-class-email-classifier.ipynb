{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:43.136992Z",
     "iopub.status.busy": "2025-06-07T11:07:43.136211Z",
     "iopub.status.idle": "2025-06-07T11:07:43.398690Z",
     "shell.execute_reply": "2025-06-07T11:07:43.398144Z",
     "shell.execute_reply.started": "2025-06-07T11:07:43.136964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:43.400362Z",
     "iopub.status.busy": "2025-06-07T11:07:43.400046Z",
     "iopub.status.idle": "2025-06-07T11:07:43.835868Z",
     "shell.execute_reply": "2025-06-07T11:07:43.835265Z",
     "shell.execute_reply.started": "2025-06-07T11:07:43.400344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:43.836836Z",
     "iopub.status.busy": "2025-06-07T11:07:43.836576Z",
     "iopub.status.idle": "2025-06-07T11:07:44.018020Z",
     "shell.execute_reply": "2025-06-07T11:07:44.017486Z",
     "shell.execute_reply.started": "2025-06-07T11:07:43.836814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:44.018890Z",
     "iopub.status.busy": "2025-06-07T11:07:44.018668Z",
     "iopub.status.idle": "2025-06-07T11:07:44.022697Z",
     "shell.execute_reply": "2025-06-07T11:07:44.022040Z",
     "shell.execute_reply.started": "2025-06-07T11:07:44.018872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:44.024635Z",
     "iopub.status.busy": "2025-06-07T11:07:44.024413Z",
     "iopub.status.idle": "2025-06-07T11:07:44.284233Z",
     "shell.execute_reply": "2025-06-07T11:07:44.283692Z",
     "shell.execute_reply.started": "2025-06-07T11:07:44.024619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:44.285131Z",
     "iopub.status.busy": "2025-06-07T11:07:44.284896Z",
     "iopub.status.idle": "2025-06-07T11:07:45.322316Z",
     "shell.execute_reply": "2025-06-07T11:07:45.321694Z",
     "shell.execute_reply.started": "2025-06-07T11:07:44.285108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Load and process dataset\n",
    "print(\"Available datasets:\")\n",
    "print(os.listdir(\"/kaggle/input/multi-classs-email-dataset\"))\n",
    "\n",
    "with open(\"/kaggle/input/multi-classs-email-dataset/email_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    emails = json.load(f)\n",
    "\n",
    "\n",
    "def format_email(e):\n",
    "    return {\n",
    "        \"text\": f\"### Subject:\\n{e['subject']}\\n\\n### Body:\\n{e['body']}\\n\\n### Labels:\\n{', '.join(e['labels'])}\"\n",
    "    }\n",
    "\n",
    "formatted_data = [format_email(e) for e in emails]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-07T11:07:45.323287Z",
     "iopub.status.busy": "2025-06-07T11:07:45.323039Z",
     "iopub.status.idle": "2025-06-07T11:09:15.275167Z",
     "shell.execute_reply": "2025-06-07T11:09:15.274397Z",
     "shell.execute_reply.started": "2025-06-07T11:07:45.323260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers accelerate datasets trl peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.727Z",
     "iopub.execute_input": "2025-06-07T11:09:15.276377Z",
     "iopub.status.busy": "2025-06-07T11:09:15.276128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./llama-3.2-1b-lora\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "lora_model_path = \"/kaggle/working/llama-3.2-1b-lora/checkpoint-711\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Attach LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "prompt = \"\"\"### Subject:\n",
    "Reminder: Upcoming Meeting with Client\n",
    "\n",
    "### Body:\n",
    "Please note that we have a scheduled meeting with the client this Thursday at 3 PM. Ensure all reports are ready and shared beforehand.\n",
    "\"\"\"\n",
    "\n",
    "output = pipe(prompt, max_new_tokens=50, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)[0]['generated_text']\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-07T11:18:22.728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r checkpoint-710.zip /kaggle/working/llama-3.2-1b-lora/checkpoint-711\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7607848,
     "sourceId": 12085530,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
